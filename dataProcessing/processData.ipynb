{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditAPI = pd.read_csv('/Users/harmyabhatt/TextMiningAndInjuryClassification/dataFiles/redditAPIScrape.csv')\n",
    "redditScrape = pd.read_csv('/Users/harmyabhatt/TextMiningAndInjuryClassification/dataFiles/redditScrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives = []\n",
    "\n",
    "for text in redditAPI['text']:\n",
    "    text = str(text)\n",
    "    for sentence in text.split('\\n'):\n",
    "        sentence = sentence.lower()\n",
    "        #remove hyperlinks\n",
    "        sentence = re.sub(r'http\\S+', ' ', sentence)\n",
    "        #keep only words\n",
    "        sentence = re.sub(r'[^a-zA-Z\\s]', ' ', sentence)\n",
    "        #remove unnecessary spaces\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        #remove leading and trailing spaces\n",
    "        sentence = sentence.strip()\n",
    "        #remove newlines\n",
    "        sentence = sentence.replace('\\n', ' ')\n",
    "        narratives.append(sentence)\n",
    "    \n",
    "\n",
    "for text in redditScrape['text']:\n",
    "    text = str(text)\n",
    "    for sentence in text.split('\\n'):\n",
    "        sentence = sentence.lower()\n",
    "        #remove hyperlinks\n",
    "        sentence = re.sub(r'http\\S+', ' ', sentence)\n",
    "        #keep only words and punctuation\n",
    "        sentence = re.sub(r'[^a-zA-Z\\s]', ' ', sentence)\n",
    "        #remove unnecessary spaces\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        #remove leading and trailing spaces\n",
    "        sentence = sentence.strip()\n",
    "        #remove newlines\n",
    "        sentence = sentence.replace('\\n', ' ')\n",
    "        narratives.append(sentence)\n",
    "\n",
    "#remove empty strings from list\n",
    "narratives = list(filter(None, narratives))\n",
    "\n",
    "#remove duplicates\n",
    "narratives = list(set(narratives))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39655\n"
     ]
    }
   ],
   "source": [
    "#filter out narratives that are too short\n",
    "narratives = [narrative for narrative in narratives if len(narrative.split()) > 5]\n",
    "#filter out narratives that are too long\n",
    "narratives = [narrative for narrative in narratives if len(narrative.split()) < 40]\n",
    "\n",
    "\n",
    "#print len(narratives)\n",
    "print (len(narratives))\n",
    "\n",
    "#save narratives to csv\n",
    "df = pd.DataFrame(narratives)\n",
    "df.to_csv('/Users/harmyabhatt/TextMiningAndInjuryClassification/dataFiles/narrativesAfterFiltering.csv', index=False, header=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
